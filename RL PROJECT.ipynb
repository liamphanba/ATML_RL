{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RL PROJECT** - **DICE GAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we consider the following “Dice Game”. The objective of the game is to make money by\n",
    "outscoring the dealer or by rolling doubles. Each round, the player starts with a score of zero. Since this\n",
    "is an episodic task, discounting is not necessary.\n",
    "\n",
    "Each turn, the player has to choose between rolling their dice, or betting money on the dealer’s roll. If\n",
    "they decide to roll themselves, they can either roll one or two dice (simultaneously), which costs them\n",
    "CHF 1 for one dice, and CHF 2 for two dice. The number(s) shown by the dice are added to the player’s\n",
    "score. If the player rolls a double (i.e., two dice showing the same numbers), they get an immediate\n",
    "bonus payout of CHF 10, independent of the rest of the game or their score. If the player reaches a\n",
    "score of 31 or more, they lose the round and have to pay CHF 10.\n",
    "\n",
    "If the player chooses to bet on the dealer’s roll, they have to specify a bet-multiplicator of 1, 2, or 3. The\n",
    "dealer then rolls their dice and the player is paid/has to pay according to the formula\n",
    "(“Player Score” −“Dealer Dice Result”) ·“Bet-Multiplicator”,\n",
    "and the round is over\n",
    "\n",
    "The player has two identical dice, showing the numbers {1,2,3,4,5,6}. The dealer has one dice,\n",
    "showing the numbers {25,26,27,28,29,30}. All three dice are weighted, such that their highest\n",
    "number has twice the probability of each of the smaller numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlowChart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Player\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Player Dices have the following setup:\n",
    "\n",
    "    P(D = 1) = p\n",
    "    P(D = 2) = p\n",
    "    P(D = 3) = p\n",
    "    P(D = 4) = p\n",
    "    P(D = 5) = p\n",
    "    P(D = 6) = 2p\n",
    "\n",
    "Therefore, since the Universe is equal to 1 by definition, p should sum to 1 as well:\n",
    "\n",
    "    p + p + p + p + p + 2p = 1 \n",
    "\n",
    "    6p + 2p = 1\n",
    "\n",
    "    8p = 1\n",
    "\n",
    "    p = 1/8\n",
    "\n",
    "The Player Dices have the following properties:\n",
    "\n",
    "    P(D = 1) = 1/8\n",
    "    P(D = 2) = 1/8\n",
    "    P(D = 3) = 1/8\n",
    "    P(D = 4) = 1/8\n",
    "    P(D = 5) = 1/8\n",
    "    P(D = 6) = 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dealer Dice have the following setup:\n",
    "\n",
    "    P(D = 25) = p\n",
    "    P(D = 26) = p\n",
    "    P(D = 27) = p\n",
    "    P(D = 28) = p\n",
    "    P(D = 29) = p\n",
    "    P(D = 30) = 2p\n",
    "\n",
    "Therefore, since the Universe is equal to 1 by definition, p should sum to 1 as well:\n",
    "\n",
    "    p + p + p + p + p + 2p = 1 \n",
    "\n",
    "    6p + 2p = 1\n",
    "\n",
    "    8p = 1\n",
    "\n",
    "    p = 1/8\n",
    "\n",
    "The Dealer Dice  have the following properties:\n",
    "\n",
    "    P(D = 1) = 1/8\n",
    "    P(D = 2) = 1/8\n",
    "    P(D = 3) = 1/8\n",
    "    P(D = 4) = 1/8\n",
    "    P(D = 5) = 1/8\n",
    "    P(D = 6) = 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout, the state space should have (at most) one terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Considering the game as a Markov decision process, identify the state space S, the action\n",
    "space A, and the reward set R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ANSWER:\n",
    "\n",
    "- State Space (S) is defined by the Player Score after a **TURN**, and thus can range from 0 to 31 \n",
    "\n",
    "- The Action Space (A) is defined by the set of possible Actions the player can choose at each **TURN**, and thus can be the following:\n",
    "   \n",
    "        1. Roll 1 dice or 2 dices simulteanously\n",
    "        2. Bet on the Dealer Roll (Specify 1,2,3 in the Bet-Multiplicator)\n",
    "\n",
    "- The Reward Set (R) can be seen as the reward for each actions, therefore the player can:\n",
    "\n",
    "    For Rolling the Dice:\n",
    "        \n",
    "        1. Gain 8CHF if 2 Dices roll have the same value (10CHF - the initial cost of 2 dices (2CHF))\n",
    "        2. Lose 1CHF if 1 Dice roll (not payout, except the initial cost of 1 dice (1CHF))\n",
    "        2. Lose -10CHF if the Player Score reach 31 and above\n",
    "    \n",
    "    For Betting on Dealer Dice:\n",
    "    \n",
    "        1. (Player Score - Dealer Dice Result) * Bet-Multiplicator CHF, \n",
    "        can be gain or loss depending on the Dealer Dice value\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a Python class that represents the game as a reinforcement learning task. The class\n",
    "should contain all the information about the game state, and should provide a “step” method that\n",
    "takes an action as input and returns the reward and next state, as well as a “reset” method that\n",
    "resets the game to its initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using dynamic programming, compute the value functions under the following policies. Explain the results and represent them graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    “R1”: The player always rolls a single dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    “R2”: The player always rolls both dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    “RR”: If the player’s score is strictly smaller than 20, they roll either one or two dice with equal\n",
    "    probability. Otherwise, they choose one of the three bet-multiplicators uniformly at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find the optimal policy using dynamic programming. Represent the action-value function under\n",
    "the optimal policy graphically. Explain the results and compare them to those of the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the class you implemented in the first task for the following Monte Carlo simulation: estimate\n",
    "the value of the initial state under each of the policies from the previous tasks (“R1”, “R2”, “RR”,\n",
    "“Optimal”). Illustrate the results and compare them to the results of the previous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
